local Runner = {}
local nil_or_empty_response =  "It's looks like there is a problem with the answer generated by the model, try with another function or in a different line."
local error_file_permission = "Error to run the model, can not write the file for the temp prompt"
local error_handle_nil = "Error to run the model, the handle is nil"
local prompt_template = [[
<|system|>
You are a code explanation expert. Analyze this code and provide a clear explanation in a few lines</s>
<|user|>
Explain this %s function:

%s</s>
<|assistant|>
]]
local error_message_template = [[
An error occurred while analyzing the code:
Error: %s

Possible solutions:
- Check if the model path is correct (%s)
- Ensure llama-cli is properly installed (%s)
- Verify the function context is valid
- Check if the language is supported

You can check the logs at: %s]]

function Runner.new(config, logger, parser)
    local self = setmetatable({}, { __index = Runner })
    self.config = config
    self.logger = logger
    self.parser = parser
    return self
end

function Runner.run_local_model(self, context, language)
    self.logger:debug("Starting function explanation", {
        language = language,
        context_length = #context
    })

    local full_prompt = string.format(prompt_template, language, context)

    self.logger:debug("Generated prompt", {prompt = full_prompt})

    local tmp_prompt = os.tmpname()
    local tmp_prompt_file = io.open(tmp_prompt, 'w')

    if not tmp_prompt_file then
        self.logger:debug(error_file_permission)
        return error_file_permission
    end

    tmp_prompt_file:write(full_prompt)
    tmp_prompt_file:close()

    local command = string.format(
    '%s -m %s --temp %d -f %s -n 512 --ctx-size %d --threads %d --top_p %d 2>>%s',
    self.config.llama_cpp_path,
    self.config.model_path,
    self.config.temp,
    tmp_prompt,
    self.config.context_window,
    self.config.num_threads,
    self.config.top_p,
    self.logger.command_path)

    self.logger:debug("Executing command", {command = command})

    local handle = io.popen(command)

    if not handle then
        self.logger:debug(error_handle_nil)
        return error_handle_nil
    end

    local result = handle:read("*a")
    local success, exit_type, exit_code = handle:close()

    self.logger:debug("Command execution completed", {
        result = result,
        success = success,
        exit_type = exit_type,
        exit_code = exit_code
    })

    os.remove(tmp_prompt)

    if result == nil or result == ''then
        self.logger:debug("The result is nil or empty")
        return nil_or_empty_response
    end

    self.logger:debug("Command result", {
        result = result
    })

    result = self.parser.clean_model_output(result)

    if result == nil or result == ''then
        self.logger:debug("Parser return nil or ''")
        return nil_or_empty_response
    end

    self.logger:debug("Parse result", {
        cleaned_length = #result,
        result = result
    })
    return result
end

function Runner.explain_function(self, language, context)
    self.logger:debug("Configuration ",self.config)
    local function execute_analysis()
        local success, result = pcall(function()
            return self:run_local_model(context, language)
        end)
        if success then
            return result, nil
        else
            local error_message = string.format(
                error_message_template,
                tostring(result),
                self.config.model_path,
                self.config.llama_cpp_path,
                self.logger.error_path)
            return nil, error_message
        end
    end
    return execute_analysis()
end

return Runner
