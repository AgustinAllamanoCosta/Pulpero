local Runner = {}
local nil_or_empty_response =
"It's looks like there is a problem with the answer generated by the model, try with a different query."

function Runner.new(config, logger, parser)
    local self = setmetatable({}, { __index = Runner })
    if config == nil then
        error("Model Runner config is nil")
    end
    if logger == nil then
        error("Model Runner logger is nil")
    end
    if parser == nil then
        error("Model Runner parser is nil")
    end

    self.config = config
    self.logger = logger
    self.parser = parser
    self.model_parameters = {
        mirostat = "2",
        context_window = self.config.context_window,
        response_size = "512",
        temp = "0.1",
        top_p = "0.3",
        num_threads = self.config.num_threads,
        model_path = self.config.model_path,
        llama_cpp_path = self.config.llama_cpp_path,
        command_debug_output = logger:get_config().command_path
    }
    return self
end

function Runner:run_local_model(tmp_prompt, config)
    local response_file = os.tmpname()

    local command = string.format(
        '%s '
        .. ' -m %s'
        .. ' --temp %s'
        .. ' --ctx-size %s'
        .. ' --threads %s'
        .. ' --top_p %s'
        .. ' --repeat-penalty 1.15'
        .. ' --repeat-last-n 32'
        .. ' --mirostat %s.05'
        .. ' --mirostat-lr 0.05.0'
        .. ' --mirostat-ent 3.0'
        .. ' -n %s'                                       -- Maximum tokens to generate
        .. ' -ngl 14'                                     -- Use GPU for better performance
        .. ' -b 256'                                      -- Batch size for processing
        .. ' -r "User:" --in-prefix " " --in-suffix "A:"' -- Better chat handling
        .. ' --tensor-split 0'                            -- Move all computation to GPU
        .. ' -f %s -no-cnv 1> %s 2> %s',                  -- Input file at the end
        config.llama_cpp_path,
        config.model_path,
        config.temp,
        config.context_window,
        config.num_threads,
        config.top_p,
        config.mirostat,
        config.response_size,
        tmp_prompt,
        response_file,
        config.command_debug_output
    )

    self.logger:debug("Executing command", { command = command })
    local success = os.execute(command)

    if not success then
        self.logger:error("Command execution failed")
        return nil_or_empty_response
    end

    local response = ""
    local response_handle = io.open(response_file, "r")
    if response_handle then
        local content = response_handle:read("*a")
        response_handle:close()
        response = content
    end

    os.remove(response_file)
    self.logger:debug("Command execution completed", { response = response })

    if response == nil or response == '' then
        self.logger:error("The result is nil or empty")
        return nil_or_empty_response
    end
    local parser_result = self.parser:clean_model_output(response)

    return { message = parser_result }
end

function Runner:talk_with_model(tmp_prompt)
    local model_response = self:run_local_model(tmp_prompt, self.model_parameters)
    return model_response.message
end

return Runner
