local prompts = require("prompts")
local json = require("JSON")
local Runner = {}
local user_key = "User"
local assistant_key = "Assistant"
local cache_prompt_path = "/tmp/prompts.bin"
local nil_or_empty_response =
"It's looks like there is a problem with the answer generated by the model, try with a different query or line of code."
local error_file_permission = "Error to run the model, can not write the file for the temp prompt"

function Runner.new(config, logger, parser, tool_manager)
    local self = setmetatable({}, { __index = Runner })
    if config == nil then
        error("Model Runner config is nil")
    end
    if logger == nil then
        error("Model Runner logger is nil")
    end
    if parser == nil then
        error("Model Runner parser is nil")
    end
    self.config = config
    self.logger = logger
    self.parser = parser
    self.chat_context = self:create_new_chat_context()
    self.tool_manager = tool_manager
    self.model_parameters = {
        mirostat = "2",
        context_window = self.config.context_window,
        response_size = "1024",
        temp = "0.3",
        top_p = "0.4",
        num_threads = self.config.num_threads,
        model_path = self.config.model_path,
        llama_cpp_path = self.config.llama_cpp_path,
        command_debug_output = logger:get_config().command_path
    }
    return self
end

function Runner.create_new_chat_context(self)
    return {
        messages = {},
        max_messages = 10,
        current_tokens = 0,
        max_tokens = self.config.context_window
    }
end

function Runner.generate_prompt_file(self, prompt)
    self.logger:debug("Creating temp file with prompt")
    local tmp_prompt = os.tmpname()
    local tmp_prompt_file = io.open(tmp_prompt, 'w')
    if not tmp_prompt_file then
        self.logger:error(error_file_permission)
        return error_file_permission
    end
    tmp_prompt_file:write(prompt)
    tmp_prompt_file:close()
    self.logger:debug("File created", { tmp_file = tmp_prompt })
    return tmp_prompt
end

function Runner.run_local_model(self, prompt, config)
    self.logger:debug("Configuration ", config)

    local tmp_prompt = self:generate_prompt_file(prompt)
    local response_file = os.tmpname()

    local command = string.format(
        '%s -m %s --temp %s --ctx-size %s --threads %s --top_p %s --repeat-penalty 1.2'
        .. ' --repeat-last-n 64 --mirostat %s --mirostat-lr 0.1 --mirostat-ent 5.0'
        .. ' -n %s'                                       -- Maximum tokens to generate
        .. ' --prompt-cache ' .. cache_prompt_path        -- Cache the prompt for faster loading
        .. ' -ngl 14'                                     -- Use GPU for better performance
        .. ' -b 1024'                                      -- Batch size for processing
        .. ' -r "User:" --in-prefix " " --in-suffix "A:"' -- Better chat handling
        .. ' --tensor-split 0'                            -- Move all computation to GPU
        .. ' -f %s -no-cnv 1> %s 2> %s',                  -- Input file at the end
        config.llama_cpp_path,
        config.model_path,
        config.temp,
        config.context_window,
        config.num_threads,
        config.top_p,
        config.mirostat,
        config.response_size,
        tmp_prompt,
        response_file,
        config.command_debug_output
    )

    self.logger:debug("Executing command", { command = command })
    local success = os.execute(command)

    if not success then
        self.logger:error("Command execution failed")
        return nil_or_empty_response
    end

    local response = ""
    local response_handle = io.open(response_file, "r")
    if response_handle then
        local content = response_handle:read("*a")
        response_handle:close()
        response = content
    end

    os.remove(tmp_prompt)
    os.remove(response_file)
    self.logger:debug("Command execution completed", { response = response })

    if response == nil or response == '' then
        self.logger:error("The result is nil or empty")
        return nil_or_empty_response
    end
    local parser_result = self.parser:clean_model_output(response)
    local tool_calls = self.tool_manager:parse_tool_calls(parser_result)
    local code = self.parser:get_code_from_response(parser_result)
    return { message = parser_result, code = code, tool_calls = tool_calls }
end

function Runner:update_chat_context(role, content)
    table.insert(self.chat_context.messages, {
        role = role,
        content = content
    })

    while #self.chat_context.messages > self.chat_context.max_messages do
        table.remove(self.chat_context.messages, 1)
    end
end

function Runner:build_chat_history()
    local history = ""
    for _, msg in ipairs(self.chat_context.messages) do
        if msg.role == user_key then
            history = history .. "User:" .. msg.content .. "\n"
        else
            history = history .. "Assistant: " .. msg.content .. "<｜end▁of▁sentence｜>"
        end
    end
    return history
end

function Runner.clear_model_cache(self)
    os.remove(cache_prompt_path)
    self.chat_context = self:create_new_chat_context()
end

function Runner.process_tool_calls(self, tool_calls, prompt)
    for _, tool_call in ipairs(tool_calls) do
        self.logger:debug("Processing tool call", { tool = tool_call.name })

        local tool_result = self.tool_manager:execute_tool(tool_call.name, tool_call.params)

        local result_str
        if tool_result.success then
            result_str = string.format(
                "<toolresult name=\"%s\" success=\"true\" result=\"%s\" />",
                tool_call.name,
                json.encode(tool_result.result)
            )
        else
            result_str = string.format(
                "<toolresult name=\"%s\" success=\"false\" error=\"%s\" />",
                tool_call.name,
                tool_result.error
            )
        end

        local pattern = string.format("<toolcall name=\"%s\" params=\".*\" />", tool_call.name)
        prompt = prompt:gsub(pattern, result_str)
    end

    return prompt
end

function Runner.create_dynamic_prompot(self, message)
    local current_chat_history = self:build_chat_history()
    local dynamic_prompt = ""
    local chat_history = ""
    local tool_descriptions = ""

    if current_chat_history ~= "" then
        chat_history = "Chat History:\n" .. current_chat_history .. "\nEnd History"
    end

    local tools = self.tool_manager:get_tool_descriptions()
    if #tools > 0 then
        for _, tool in ipairs(tools) do
            tool_descriptions = tool_descriptions .. string.format(
                "- Tool Name: \"%s\"\nDescription: \"%s\"\nParameters: \"%s\"\n\n",
                tool.name,
                tool.description,
                json.encode(tool.parameters)
            )
        end
    end

    dynamic_prompt = string.format(prompts.chat_with_functions_calls, tool_descriptions, os.date("%x", os.time()),
        chat_history, message)
    return dynamic_prompt
end

function Runner.talk_with_model(self, user_message)
    local dynamic_prompt = self:create_dynamic_prompot(user_message)
    local model_response = self:run_local_model(dynamic_prompt, self.model_parameters)
    if model_response.tool_calls ~= nil and #model_response.tool_calls > 0 then
        self.logger:debug("Tools call found")
        local new_prompt = self:process_tool_calls(model_response.tool_calls, dynamic_prompt)
        model_response = self:run_local_model(new_prompt, self.model_parameters)
    end
    self:update_chat_context(assistant_key, model_response.message)
    return model_response.message, model_response.code
end

return Runner
