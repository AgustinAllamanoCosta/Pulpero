local Runner = {}

local UI = require('pulpero.ui')
local Logger = require('pulpero.logger')
local Parser = require('pulpero.parser')
local nil_or_empty_response =  "It's looks like there is a problem with the answer generated by the model, try with another function or in a different line."

function Runner.new(config)
    local self = setmetatable({}, { __index = Runner })
    self.config = config
    self.logger = Logger.new(config.logs)
    self.parser = Parser.new(config)
    self.ui = UI.new(config)
    return self
end

function Runner.run_local_model(self, context, language)
    self.logger:debug("Starting function explanation", {
        language = language,
        context_length = #context
    })

    local prompt = string.format([[
<|system|>
You are a code explanation expert. Analyze this code and provide a clear explanation in a few lines, max 10</s>
<|user|>
Explain this %s function:

%s</s>
<|assistant|>
]], language, context)

    self.logger:debug("Generated prompt", {prompt = prompt})

    local tmp_prompt = os.tmpname()
    local tmp_prompt_file = io.open(tmp_prompt, 'w')

    if not tmp_prompt_file then
        self.logger:debug("Error to run the model, can not write the file for the temp prompt")
        return "Error to run the model, can not write the file for the temp prompt"
    end

    tmp_prompt_file:write(prompt)
    tmp_prompt_file:close()

    local command_output = string.format('%s/%s',self.config.logs.directory, self.config.logs.command_output)

    local command = string.format(
        '%s -m %s --temp %d -f %s -n 512 --ctx-size %d --threads %d --top_p %d 2>>%s',
        self.config.llama_cpp_path,
        self.config.model_path,
        self.config.temp,
        tmp_prompt,
        self.config.context_window,
        self.config.num_threads,
        self.config.top_p,
        command_output
    )

    self.logger:debug("Executing command", {command = command})

    local handle = io.popen(command)

    if not handle then
        self.logger:debug("Error to run the model, the handle is nil")
        return "Error to run the model, the handle is nil"
    end

    local result = handle:read("*a")
    local success, exit_type, exit_code = handle:close()

    self.logger:debug("Command execution completed", {
        result = result,
        success = success,
        exit_type = exit_type,
        exit_code = exit_code
    })

    os.remove(tmp_prompt)

    if result == nil or result == ''then
        self.logger:debug("The result is nil or empty")
        return nil_or_empty_response
    end

    self.logger:debug("Command result", {
        result = result
    })

    result = self.parser.clean_model_output(result)

    if result == nil or result == ''then
        self.logger:debug("Parser return nil or ''")
        return nil_or_empty_response
    end

    self.logger:debug("Parse result", {
        cleaned_length = #result,
        result = result
    })

    return result
end

function Runner.explain_function(self)
    self.logger:clear_logs()
    self.logger:debug("Configuration ",self.config)
    local language = vim.bo.filetype
    if not self.config.supported_languages[language] then
        self.ui.show_error("Language not supported: " .. language)
        return
    end

    local loading = self.ui.create_loading_animation()

    loading.start()
    vim.schedule(function()
        local success, result = pcall(function()
            local context = self.parser:extract_function_context()
            return self:run_local_model(context, language)
        end)

        loading.stop()

        if success then
            self.ui.show_explanation(result)
        else
            local error_message = string.format([[
An error occurred while analyzing the code:
Error: %s

Possible solutions:
- Check if the model path is correct (%s)
- Ensure llama-cli is properly installed (%s)
- Verify the function context is valid
- Check if the language is supported

You can check the logs at: %s]],
                tostring(result),
                self.config.model_path,
                self.config.llama_cpp_path,
                self.config.logs.directory
            )
            self.ui.show_error(error_message)
        end
    end)
end

return Runner
