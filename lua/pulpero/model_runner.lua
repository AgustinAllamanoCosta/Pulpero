local Runner = {}

local nil_or_empty_response =  "It's looks like there is a problem with the answer generated by the model, try with another function or in a different line."
local error_file_permission = "Error to run the model, can not write the file for the temp prompt"
local error_handle_nil = "Error to run the model, the handle is nil"

function Runner.new(config, logger, parser, ui)
    local self = setmetatable({}, { __index = Runner })
    self.config = config
    self.logger = logger
    self.parser = parser
    self.ui = ui
    return self
end

function Runner.run_local_model(self, context, language)
    self.logger:debug("Starting function explanation", {
        language = language,
        context_length = #context
    })

    local full_prompt = string.format([[
<|system|>
You are a code explanation expert. Analyze this code and provide a clear explanation in a few lines</s>
<|user|>
Explain this %s function:

%s</s>
<|assistant|>
]], language, context)

    self.logger:debug("Generated prompt", {prompt = full_prompt})

    local tmp_prompt = os.tmpname()
    local tmp_prompt_file = io.open(tmp_prompt, 'w')

    if not tmp_prompt_file then
        self.logger:debug(error_file_permission)
        return error_file_permission
    end

    tmp_prompt_file:write(full_prompt)
    tmp_prompt_file:close()

    local command_output = string.format('%s/%s',self.config.logs.directory, self.config.logs.command_output)

    local command = string.format(
    '%s -m %s --temp %d -f %s -n 512 --ctx-size %d --threads %d --top_p %d 2>>%s',
    self.config.llama_cpp_path,
    self.config.model_path,
    self.config.temp,
    tmp_prompt,
    self.config.context_window,
    self.config.num_threads,
    self.config.top_p,
    command_output)

    self.logger:debug("Executing command", {command = command})

    local handle = io.popen(command)

    if not handle then
        self.logger:debug(error_handle_nil)
        return error_handle_nil
    end

    local result = handle:read("*a")
    local success, exit_type, exit_code = handle:close()

    self.logger:debug("Command execution completed", {
        result = result,
        success = success,
        exit_type = exit_type,
        exit_code = exit_code
    })

    os.remove(tmp_prompt)

    if result == nil or result == ''then
        self.logger:debug("The result is nil or empty")
        return nil_or_empty_response
    end

    self.logger:debug("Command result", {
        result = result
    })

    result = self.parser.clean_model_output(result)

    if result == nil or result == ''then
        self.logger:debug("Parser return nil or ''")
        return nil_or_empty_response
    end

    self.logger:debug("Parse result", {
        cleaned_length = #result,
        result = result
    })

    return result
end

function Runner.explain_function(self)
    self.logger:debug("Configuration ",self.config)

    self.ui:start_spiner()
    vim.schedule(function()
        local success, result = pcall(function()
            local language = vim.bo.filetype
            local context = self.parser:get_visual_selection()
            return self:run_local_model(context, language)
        end)

        self.ui:stop_spiner()

        if success then
            self.ui:show_explanation(result)
        else
            local error_message = string.format([[
An error occurred while analyzing the code:
Error: %s

Possible solutions:
- Check if the model path is correct (%s)
- Ensure llama-cli is properly installed (%s)
- Verify the function context is valid
- Check if the language is supported

You can check the logs at: %s]],
            tostring(result),
            self.config.model_path,
            self.config.llama_cpp_path,
            self.config.logs.directory
            )
            self.ui:show_error(error_message)
        end
    end)
end

return Runner
